{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c9805966-0f48-4c67-8905-233f499d05e1",
   "metadata": {},
   "source": [
    "# Custom LoRA: Finetuning LLaMA3-8B-Instruct\n",
    "---\n",
    "\n",
    "**Overview:** \n",
    "\n",
    "In this notebook, we will explore the process of fine-tuning the LLaMA 3 model using Low-Rank Adaptation (LoRA). The topics covered include:\n",
    "\n",
    "1. **Data Preprocessing**: Preparing and cleaning the data to ensure optimal input for the model training.\n",
    "2. **Training Setup**:\n",
    "   - **Loading the Tokenizer**: Initializing the tokenizer suited for LLaMA 3 to process the text data.\n",
    "   - **Setting Train Hyperparameters**: Defining the key hyperparameters for effective model training.\n",
    "   - **PEFT (Parameter-Efficient Fine-Tuning)**: Techniques to fine-tune the model with fewer resources.\n",
    "   - **Quantization**: Reducing the model size while maintaining performance, allowing faster inference and reduced memory usage.\n",
    "   - **Loading the Pre-trained Model**: Bringing in the pre-trained LLaMA 3 model to further fine-tune it with custom data.\n",
    "   - **Setting the Trainer Hyperparameters**: Configuring parameters specific to the training loop for optimal performance.\n",
    "   - **Run Inference**: Evaluating the fine-tuned model on new data to test its performance and accuracy.\n",
    "\n",
    "This notebook will guide you through the steps of effectively fine-tuning and optimizing the LLaMA 3 model for your specific use case."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aceef47a",
   "metadata": {},
   "source": [
    "## Data Preprocessing\n",
    "We would use the openassistant guanaco dataset for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d637021",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcd48b2c-70ab-473d-9e5d-b62f135ad096",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ds = load_dataset(\"timdettmers/openassistant-guanaco\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db47e3b8-f287-454e-b24d-20ab646c8f00",
   "metadata": {},
   "source": [
    "Let us view 5 samples from the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76f71948-6334-4201-99da-13df1f1aaa6d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for sample in ds['train'].select(range(5)):\n",
    "    print(f\"\\n {'*' * 64}\\n{sample}\\n{'*' * 64}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cb06a8e-f405-4de2-965b-7f86ff042a74",
   "metadata": {},
   "source": [
    "We can see dict objects with `text` key in multiple languages. For this lab, we limit ourselves to the english samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdbcfd27-1fb5-4fcf-8548-899f879487bf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langdetect import detect\n",
    "\n",
    "def remove_nonEnglish_rows(ds):\n",
    "    # Initialize an empty list to store rows detected as English\n",
    "    new_ds = []\n",
    "    \n",
    "    # Initialize a list to store indices of rows that cause issues (corner cases)\n",
    "    corner_case = []\n",
    "    \n",
    "    # Iterate through each row in the dataset's 'text' column\n",
    "    for i, row in enumerate(ds['text']):\n",
    "        try:\n",
    "            # Detect the language of the text\n",
    "            if detect(str(row)) == 'en':\n",
    "                # If the language is English, add the row to new_ds\n",
    "                new_ds.append(row)\n",
    "        except:\n",
    "            # If an exception occurs, add the index to corner_case\n",
    "            corner_case.append(i)\n",
    "    \n",
    "    # Return the list of English rows and the indices of corner cases\n",
    "    return new_ds, corner_case\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e8a6083-0afc-418b-8349-d6a8a540f887",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "filter_train_samples,cc_train = remove_nonEnglish_rows(ds['train'])\n",
    "\n",
    "print(\"Count of training samples: \",len(filter_train_samples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f69abf96",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "filter_test_samples,cc_test = remove_nonEnglish_rows(ds['test'])\n",
    "print(\"Count of testing samples: \",len(filter_test_samples))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "318e2b02-2f3a-49c7-a0bf-d8cf8f7aa2b2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# save English text samples\n",
    "import json\n",
    "def save_jsonl(ds,filename):\n",
    "    with open(f\"data/{filename}.jsonl\", \"w\") as write_file:\n",
    "            json.dump(ds, write_file, indent=4)\n",
    "            print(\"dataset saved in jsonl format ....\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a730f75d-d4d4-440a-a740-d60a78ed1ee4",
   "metadata": {},
   "source": [
    "The LLaMA3 models expect the train datasets to be in a specific format as listed [here](https://llama.meta.com/docs/model-cards-and-prompt-formats/meta-llama-3/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2011142",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def transform_to_template(example):\n",
    "    conversation_text = example['text']\n",
    "    segments = conversation_text.split(\"###\")[1:]\n",
    "    \n",
    "\n",
    "    for idx,segment in enumerate(segments):\n",
    "        if idx%2==0:\n",
    "            segments[idx] = segment.replace('Human:',\"<|start_header_id|>user<|end_header_id|>\") + \"<|eot_id|>\"\n",
    "        else:\n",
    "            segments[idx] = segment.replace('Assistant:',\"<|start_header_id|>assistant<|end_header_id|>\") + \"<|eot_id|>\"\n",
    "    \n",
    "    \n",
    "\n",
    "    segments = [\"<|begin_of_text|><|start_header_id|>system<|end_header_id|> You are a helpful assistant<|eot_id|>\"] + segments\n",
    "\n",
    "    return {'text': ''.join(segments)}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6499445-f17b-49b8-b88a-2193f01ecf82",
   "metadata": {},
   "source": [
    "We will make a seperate folder for storing these datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cdaf782-01d9-4a0a-b4de-ae8b6b0a302b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "! mkdir -p data\n",
    "! mkdir -p data/filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ac79126",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# set file names  \n",
    "save_train_filename = 'filtered/train'\n",
    "save_test_filename = 'filtered/test'\n",
    "\n",
    "# save file\n",
    "save_jsonl(filter_train_samples, save_train_filename)\n",
    "save_jsonl(filter_test_samples, save_test_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d419c200",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset = load_dataset('data/filtered/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09f7ca8b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "template_dataset = dataset.map(transform_to_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0520bc61",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!mkdir -p data/ds_preprocess\n",
    "template_dataset.save_to_disk('data/ds_preprocess/')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca1ed783-501c-40a7-892d-75c7c560aaf1",
   "metadata": {},
   "source": [
    "We have now saved the transformed dataset. This would help in loading the dataset directly in case of kernel failures."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "202a46e2",
   "metadata": {},
   "source": [
    "## Training Setup\n",
    "\n",
    "The following libraries would be helpful in setting up the finetuning process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0fe928a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# In some cases where you have access to limited computing resources, you might have to uncomment os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:64\" if you run into not enough memory issue \n",
    "\n",
    "import os\n",
    "import torch\n",
    "import json\n",
    "import re\n",
    "\n",
    "from peft import LoraConfig, PeftModel\n",
    "from trl import SFTTrainer\n",
    "from datasets import load_dataset, load_from_disk\n",
    "from langdetect import detect\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    TrainingArguments,\n",
    "    pipeline,\n",
    "    logging,\n",
    ")\n",
    "\n",
    "# os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:64\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a13d9f4e-7581-4a91-a679-6fca569e741c",
   "metadata": {},
   "source": [
    "Setting up the important paths for loading and saving important artifacts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "185962e4-4124-4690-b813-5cba5bf007ff",
   "metadata": {},
   "source": [
    "Llama-3 family of models are open source but require an access request approval. For the bootcamp environment, the weights have already been converted to huggingface compatible format and stored at a shared location for quicker access for the participants. \n",
    "\n",
    "In case of running the material on your own environment, please request access for Llama models from [here](https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct) and generate your HuggingFace user access token from this [link](https://huggingface.co/settings/tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd3eaf64-a348-4b8b-9bd5-32f26f857d5a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# initialize path to the base model \n",
    "# base_model = \"meta-llama/Meta-Llama-3-8B-Instruct\" # Use this while running the material in your own standalone environment.\n",
    "\n",
    "base_model = \"/local/llama3_8b_weights/\" # shared model weight location\n",
    "\n",
    "# set the path to the dataset template\n",
    "data_path = \"data/ds_preprocess/train\"\n",
    "# set the path to the dataset template\n",
    "eval_path = \"data/ds_preprocess/test\"\n",
    "\n",
    "# load the transformed dataset\n",
    "dataset = load_from_disk(data_path)\n",
    "eval_dataset = load_from_disk(eval_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4214251b-0be0-4727-b956-b17cdb95e99d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Needed for standalone run\n",
    "token= '<INSERT_HUGGINGFACE_TOKEN_HERE>'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d1c3de7-27d3-48b2-9ae6-e12a26c432aa",
   "metadata": {},
   "source": [
    "### Loading the Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdee1d25",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#The below code should be used to download the tokenizer for the Llama-3-8V-Instruct model, once you get the permissions.\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model,\n",
    "                                          token=token,\n",
    "                                          trust_remote_code=True\n",
    "                                         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5522772f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d17b9699-7e15-47a4-86f8-0ff73ae69bb8",
   "metadata": {},
   "source": [
    "We would also need a directory to save the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5d624d7-8727-47c7-87c5-646161fd4510",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "! mkdir -p model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba8f953f-4e85-4bc3-a78c-6884a97af10f",
   "metadata": {},
   "source": [
    "### Setting Train Hyper Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2da5298",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "training_params = TrainingArguments(\n",
    "    output_dir=\"model/results\",             # Directory to save the model results\n",
    "    num_train_epochs=2,                     # Number of training epochs\n",
    "    per_device_train_batch_size=5,          # Batch size per device during training\n",
    "    gradient_accumulation_steps=4,          # Number of steps to accumulate gradients\n",
    "    group_by_length=True,                   # Group sequences of similar lengths together\n",
    "    save_steps=100,                         # Save model checkpoint every 100 steps\n",
    "    logging_steps=25,                       # Log metrics every 25 steps\n",
    "    learning_rate=2e-4,                     # Initial learning rate\n",
    "    weight_decay=0.001,                     # Weight decay to apply (L2 regularization)\n",
    "    fp16=False,                             # Use 16-bit precision (half-precision) during training\n",
    "    bf16=False,                             # Use bfloat16 precision\n",
    "    max_grad_norm=0.3,                      # Maximum gradient norm (for gradient clipping)\n",
    "    max_steps=-1,                           # Total number of training steps (-1 means no limit)\n",
    "    warmup_ratio=0.03,                      # Ratio of steps to perform learning rate warmup\n",
    "    optim=\"paged_adamw_32bit\",              # Optimizer to use (32-bit AdamW with paged memory)\n",
    "    lr_scheduler_type=\"constant\",           # Learning rate scheduler type (constant)\n",
    "    report_to=\"tensorboard\"                 # Reporting tool (TensorBoard in this case)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d25da6c-f7f0-4a46-ace7-66d02ee6f300",
   "metadata": {},
   "source": [
    "### PEFT\n",
    "LoRA techniques are applied through `LoraConfig`, which provides PEFT parameters that control how the method is applied to the base model. A description of the parameter used in the cell below is given as follows:\n",
    "\n",
    "- **lora_alpha**: LoRA scaling factor\n",
    "- **lora_dropout**: The dropout probability for LoRA layers.\n",
    "- **r**: the rank of the update matrices, expressed in int. Lower rank results in smaller update matrices with fewer trainable parameters.\n",
    "- **bias**: Specifies if the bias parameters should be trained. It can be 'none', 'all', or 'lora_only'.\n",
    "- **task_type**: Possible task types which include `CAUSAL_LM`, `FEATURE_EXTRACTION`, `QUESTION_ANS`, `SEQ_2_SEQ_LM`, and `SEQ_CLS and TOKEN_CLS`.   \n",
    "\n",
    "Because the task we want to perform is text generation, we have set the task_type to Causal language model `(CAUSAL_LM)`, which is frequently used for text generation tasks. Please run the cell below to set up the LoRA configuration. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "743a2ce3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "peft_params = LoraConfig(\n",
    "    lora_alpha=16,                # Alpha parameter for Lora scaling\n",
    "    lora_dropout=0.1,             # Dropout rate for Lora layers\n",
    "    r=64,                         # Rank of the Lora matrices\n",
    "    bias=\"none\",                  # Type of bias to apply (none in this case)\n",
    "    task_type=\"CAUSAL_LM\",        # Type of task (Causal Language Modeling in this case)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0cf32ac-6bb0-4763-b59e-c1c0b65e6138",
   "metadata": {},
   "source": [
    "### Quantization\n",
    "**4-bit quantization configuration**\n",
    "\n",
    "Model quantization is a popular deep-learning optimization method in which model data—network parameters and activations—are converted from floating-point to lower-precision representation, typically using 8-bit integers. Quantization represents data with fewer bits, making it a useful technique for reducing memory usage and accelerating inference, especially in large language models (LLMs). It can be combined with PEFT methods to make it easier to train and load LLMs for inference.\n",
    "\n",
    "<center><img src=\"imgs/quantization.png\" height=\"400px\" width=\"700px\" /></center>\n",
    "<center> <a href=\"https://developer.nvidia.com/blog/achieving-fp32-accuracy-for-int8-inference-using-quantization-aware-training-with-tensorrt/\" > source: Using Quantization Aware Training with NVIDIA TensorRT</a></center>\n",
    "\n",
    "Several ways and algorithms to quantize a model including can be found [here](https://huggingface.co/docs/peft/main/en/developer_guides/quantization). A library to easily implement quantization and integrate with transformers is the `bitsandbytes` library. The library provides config parameters to quantize a model to 8 or 4 bits using the `BitsAndBytesConfig` class. The 4 bits parameters used in the cell below are described as follows:\n",
    "\n",
    "- **load_in_4bit**: set `True` to quantize the model to 4-bits when you load it\n",
    "- **bnb_4bit_quant_type**: set to `\"nf4\"` to use a special 4-bit data type for weights initialized from a normal distribution\n",
    "- **bnb_4bit_use_double_quant**: set `True` to use a nested quantization scheme to quantize the already quantized weights\n",
    "- **bnb_4bit_compute_dtype**: set to `torch.float16` or `torch.bfloat16` to use bfloat16 for faster computation \n",
    "\n",
    "Run the cell below to set the 4-bit quantization for our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a360d31b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "quant_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    bnb_4bit_use_double_quant=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e04e9e79-dcaa-4459-970d-a89389037c4b",
   "metadata": {},
   "source": [
    "### Loading the pre-trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b355ece",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model,\n",
    "    quantization_config=quant_config,\n",
    "    device_map={\"\": 0},\n",
    "    token=token\n",
    ")\n",
    "model.config.use_cache = False\n",
    "model.config.pretraining_tp = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f31ee3c8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70856c8c-0ede-4a08-ac8f-45e71a3826c3",
   "metadata": {},
   "source": [
    "We can see the model is loaded on the GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "523b4407-717c-436a-a773-e4e4de8b15be",
   "metadata": {},
   "source": [
    "### Set the Trainer Hyperparameters\n",
    "\n",
    "To initiate our model trainer, we create a trainer object from [Supervised fine-tuning (SFT)](https://huggingface.co/docs/trl/en/sft_trainer). SFT is part of the integrated transformer [Reinforcement Learning (TRL)](https://huggingface.co/docs/trl/en/index) tools used to train transformer language models using Reinforcement Learning. Others include [Reward Modeling step (RM)](https://huggingface.co/docs/trl/en/reward_trainer) and  Proximal [Policy Optimization (PPO)](https://arxiv.org/abs/1707.06347). In our SFT trainer object, we set our model, training dataset, PEFT config  object, model tokenizer, and training argument parameter. We also specify the field (`text`) to use within our dataset.\n",
    "\n",
    "**Note:** *If running on a single DGX A100 GPU, modify the value of `max_seq_length` to 1024 or set it to none (as default).*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4e7d8e3-ed7e-414a-8cb3-c1c0df0a5811",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fec6c81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The notebook uses a 1000 random samples for training in the interest of time.\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=dataset.shuffle(seed=42).select(range(1000)),\n",
    "    eval_dataset = eval_dataset.select(range(len(eval_dataset))),\n",
    "    dataset_text_field=\"text\",\n",
    "    peft_config=peft_params,\n",
    "    args=training_params,\n",
    "    max_seq_length=1024,\n",
    "    packing=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdbb4801",
   "metadata": {},
   "source": [
    "Run the cell below to train the SFT trainer object. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7c65d31-eaa3-44f2-b360-75f07ba38943",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jinja2\n",
    "print(jinja2.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08b6b9d7",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa0ec40c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model\n",
    "new_model = \"model/Llama-3-8b-instruct-hf-finetune\"\n",
    "trainer.model.save_pretrained(new_model)\n",
    "trainer.tokenizer.save_pretrained(new_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c58e39c8-181c-4dc2-8b9d-de8033c4a29f",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.model.save_pretrained(new_model,safe_serialization=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7148c67c-b6ea-40aa-90e0-40a141e6a792",
   "metadata": {},
   "source": [
    "### Run Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbe7bddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac6b8210",
   "metadata": {},
   "outputs": [],
   "source": [
    "pp= pipeline(model=model, tokenizer=tokenizer, max_length=200, task=\"text-generation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9f44df2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pp(\"what has research identified in potential monopsonies\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3c87d95",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pp(\"what are some artists similar to Dvorak?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a81841d-42e1-4649-acb9-258f4ca1513f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PeftModel.from_pretrained(model, new_model)\n",
    "model = model.merge_and_unload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b160a4b3-3011-4778-8586-622a8c50cf34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reload tokenizer to save it\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model, trust_remote_code=True)\n",
    "tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64814aa0-bddf-4475-8b2d-daa886583a33",
   "metadata": {},
   "outputs": [],
   "source": [
    "pp= pipeline(model=model, tokenizer=tokenizer, max_length=200, task=\"text-generation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c41fa15e-5d63-4298-9aeb-1f7a3394b07f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba574f9b-554b-4619-a213-abea0c6a7fa9",
   "metadata": {},
   "source": [
    "The below command kills the current kernel so as to free up the GPU for running NIMs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d31e68ff-b9d3-497a-bd74-8ba12f2419a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "!kill -9 $(nvidia-smi --query-compute-apps=pid --format=csv,noheader | awk 'NR==1')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62dc639d-9f12-4bbb-9c6f-a37a8dfee007",
   "metadata": {},
   "source": [
    "To use a custom LoRA adapter, you can head over to the <a href=\"nim_lora_adapter.ipynb\"> nim_lora_adapter</a> notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ed86fc2-ba43-4b8a-84d5-64c91d49f6d1",
   "metadata": {},
   "source": [
    "---\n",
    "## Licensing\n",
    "\n",
    "Copyright © 2024 OpenACC-Standard.org. This material is released by OpenACC-Standard.org, in collaboration with NVIDIA Corporation, under the Creative Commons Attribution 4.0 International (CC BY 4.0). These materials include references to hardware and software developed by other entities; all applicable licensing and copyrights apply."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Bootcamp_Kernel",
   "language": "python",
   "name": "bootcamp_kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
