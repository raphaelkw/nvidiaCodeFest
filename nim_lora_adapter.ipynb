{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5bfdf4ac-e367-4bf6-afd3-9012128e0128",
   "metadata": {},
   "source": [
    "<p> <center> <a href=\"../../Start-NIM-RAG.ipynb\">Home Page</a> </center> </p>\n",
    "\n",
    "<div>\n",
    "    <span style=\"float: left; width: 33%; text-align: left;\"><a href=\"locally_deployed_nim.ipynb\">Previous Notebook</a></span>\n",
    "    <span style=\"float: left; width: 34%; text-align: center;\">\n",
    "        <a href=\"rag_nim_endpoints.ipynb\">1</a>\n",
    "        <a href=\"locally_deployed_nim.ipynb\">2</a>\n",
    "        <a>3</a>\n",
    "        <!-- <a href=\"challenge.ipynb\">4</a> -->\n",
    "    </span>\n",
    "    <!-- <span style=\"float: left; width: 33%; text-align: right;\"><a href=\"challenge.ipynb\">Next Notebook</a></span> -->\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "767809c7-c3c0-40fe-9783-2d41a072d4a8",
   "metadata": {},
   "source": [
    "# Running NIM With LoRA Adapters\n",
    "---\n",
    "\n",
    "**Overview:** The lab aims to introduce the concepts of Parameter Efficient Fine Tuning with a focus on LoRA adapters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb586235-1ca1-406f-b865-686bf96dff2b",
   "metadata": {},
   "source": [
    "## Low Rank Adapters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97278d93-4d12-4867-89bf-83419ef0f576",
   "metadata": {},
   "source": [
    "Full fine-tuning (that is, updating all parameters of the model) for the largest LLMs can be difficult due to the amount of computational infrastructure required to learn across the whole model.  Infrastructure costs are also increased at deployment time, where users are required to either host multiple large models in memory or tolerate increased latency as entire models are swapped in and out. Hence, the need is to achieve the objective with minimal number of changes and read & write to the weights.\n",
    "\n",
    "A popular method is to use a state-of-the-art [Parameter-Efficient Finetuning (PEFT)](https://github.com/huggingface/peft/tree/main) approach. [PEFT](https://arxiv.org/abs/2305.16742) allows finetuning a small number of (extra) model parameters instead of all the model's parameters, and this significantly decreases the computational and storage costs. One of the ways to implement PEFT is to adopt the Low-Rank Adaptation (LoRA) technique. Lora makes finetuning more efficient by greatly reducing the number of trainable parameters for downstream tasks. It does this by freezing the pre-trained model weights and injecting trainable rank decomposition matrices into each layer of the Transformer architecture. According to the [authors of LoRA](https://arxiv.org/abs/2106.09685), Aside from reducing the number of trainable parameters by 10k times, it also reduces the GPU consumption by 3x, thus delivering high throughput with no inference latency. For quick background on LoRA, please follow this [link](https://huggingface.co/docs/peft/main/en/conceptual_guides/lora).\n",
    "\n",
    "<center><img src=\"imgs/lora-arch.png\" height=\"500px\" width=\"900px\"  /></center>\n",
    "<center>  LoRA reparametrization and Weight merging. <a href=\"https://huggingface.co/docs/peft/main/en/conceptual_guides/lora\"> View source</a> </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2778513c-58ab-44f6-90e5-c9ed1532ad32",
   "metadata": {},
   "source": [
    "## NIMs with LoRA support\n",
    "\n",
    "In order to utilise the multi-faceted advantages of LoRA, NIMs allow for incorporation of multiple LoRA instances that can be placed at the top of the base model. This helps with custom tailored responses to multiple genres of user applications while using the same backbone, like an example illustration below:\n",
    "\n",
    "\n",
    "<center><img src=\"imgs/nvidia-nim-dynamic-lora-architecture.png\" height=\"500px\" width=\"900px\"  /></center>\n",
    "<center>  LoRA reparametrization and Weight merging. <a href=\"https://huggingface.co/docs/peft/main/en/conceptual_guides/lora\"> View source</a> </center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64efc225-42f0-4f6e-a386-218bddfa85ea",
   "metadata": {},
   "source": [
    "## Model Profiles\n",
    "\n",
    "NIMs are designed to deliver the best performance extractable from the compute resources. It is done via optimized model profiles specifically tuned based on available hardware (i.e., the architecture and number of GPUs), requirements, etc. A complete list can be found [here](https://docs.nvidia.com/nim/large-language-models/latest/support-matrix.html#supported-lora-formats). \n",
    "\n",
    "Let's try to list the available model profiles for our previously downloaded `Llama3-8b` NIM. Before that we would set our port \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "008cc4ce-707a-4b5f-940f-f4f583cc2fe4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import socket\n",
    "import os\n",
    "def find_available_port(start=11000, end=11999):\n",
    "    while True:\n",
    "        # Randomly select a port between start and end range\n",
    "        port = random.randint(start, end)\n",
    "        \n",
    "        # Try to create a socket and bind to the port\n",
    "        with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as sock:\n",
    "            try:\n",
    "                sock.bind((\"localhost\", port))\n",
    "                # If binding is successful, the port is free\n",
    "                return port\n",
    "            except OSError:\n",
    "                # If binding fails, the port is in use, continue to the next iteration\n",
    "                continue\n",
    "\n",
    "# Find and print an available port\n",
    "os.environ['CONTAINER_PORT'] = str(find_available_port())\n",
    "print(f\"Your have been alloted the available port: {os.environ['CONTAINER_PORT']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb5ff8ff-bfa9-4ccf-9834-07de38c4f3d3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "! docker run -it --rm --gpus all --shm-size=16GB   -u $(id -u) -p $CONTAINER_PORT:8000  nvcr.io/nim/meta/llama3-8b-instruct:1.0.0 list-model-profiles"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2227c81-367f-4331-bb2f-5ad00416d45c",
   "metadata": {},
   "source": [
    "**Likely Output:**\n",
    "\n",
    "```text\n",
    "===========================================\n",
    "== NVIDIA Inference Microservice LLM NIM ==\n",
    "===========================================\n",
    "\n",
    "NVIDIA Inference Microservice LLM NIM Version 1.0.0\n",
    "Model: nim/meta/llama3-8b-instruct\n",
    "\n",
    "Container image Copyright (c) 2016-2024, NVIDIA CORPORATION & AFFILIATES. All rights reserved.\n",
    "\n",
    "This NIM container is governed by the NVIDIA AI Product Agreement here:\n",
    "https://www.nvidia.com/en-us/data-center/products/nvidia-ai-enterprise/eula/.\n",
    "A copy of this license can be found under /opt/nim/LICENSE.\n",
    "\n",
    "The use of this model is governed by the AI Foundation Models Community License\n",
    "here: https://docs.nvidia.com/ai-foundation-models-community-license.pdf.\n",
    "\n",
    "ADDITIONAL INFORMATION: Meta Llama 3 Community License, Built with Meta Llama 3. \n",
    "A copy of the Llama 3 license can be found under /opt/nim/MODEL_LICENSE.\n",
    "\n",
    "SYSTEM INFO\n",
    "- Free GPUs:\n",
    "  -  [20b2:10de] (0) NVIDIA A100-SXM4-80GB (A100 80GB) [current utilization: 1%]\n",
    "MODEL PROFILES\n",
    "- Compatible with system and runnable:\n",
    "  - 751382df4272eafc83f541f364d61b35aed9cce8c7b0c869269cea5a366cd08c (tensorrt_llm-a100-fp16-tp1-throughput)\n",
    "  - 8835c31752fbc67ef658b20a9f78e056914fdef0660206d82f252d62fd96064d (vllm-fp16-tp1)\n",
    "  - With LoRA support:\n",
    "    - cce57ae50c3af15625c1668d5ac4ccbe82f40fa2e8379cc7b842cc6c976fd334 (tensorrt_llm-a100-fp16-tp1-throughput-lora)\n",
    "    - 8d3824f766182a754159e88ad5a0bd465b1b4cf69ecf80bd6d6833753e945740 (vllm-fp16-tp1-lora)\n",
    "- Incompatible with system:\n",
    "  - dcd85d5e877e954f26c4a7248cd3b98c489fbde5f1cf68b4af11d665fa55778e (tensorrt_llm-h100-fp8-tp2-latency)\n",
    "  - f59d52b0715ee1ecf01e6759dea23655b93ed26b12e57126d9ec43b397ea2b87 (tensorrt_llm-l40s-fp8-tp2-latency)\n",
    " ...\n",
    "  \n",
    "  - 3bdf6456ff21c19d5c7cc37010790448a4be613a1fd12916655dfab5a0dd9b8e (tensorrt_llm-h100-fp16-tp1-throughput-lora)\n",
    "  - 388140213ee9615e643bda09d85082a21f51622c07bde3d0811d7c6998873a0b (tensorrt_llm-l40s-fp16-tp1-throughput-lora)\n",
    "  - c5ffce8f82de1ce607df62a4b983e29347908fb9274a0b7a24537d6ff8390eb9 (vllm-fp16-tp2-lora)\n",
    "\n",
    "```\n",
    "<br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa927d64-8974-40d9-9bdb-cb15c97920e8",
   "metadata": {},
   "source": [
    "Let's take up a sample profile: `cce57ae50c3af15625c1668d5ac4ccbe82f40fa2e8379cc7b842cc6c976fd334 (tensorrt_llm-a100-fp16-tp1-throughput-lora)`\n",
    "\n",
    "The description tell us the details of the NIMs model profile, in our example the profile indicates that the optimisations happen via:\n",
    "- `tensorrt_llm` engines\n",
    "- for Nvidia `A100` GPU\n",
    "- with `fp16` precision\n",
    "- to be run on a single `tensor parallel` (aka single device)\n",
    "- with a focus on `throughput` and\n",
    "- support for `lora`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46adcc7e-a62c-4cc2-84a0-e0b36f3fb15a",
   "metadata": {},
   "source": [
    "### Running NIM with a model profile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ad5aaeb-e289-477c-9672-ba7d0f1ba61c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import getpass\n",
    "\n",
    "if not os.environ.get(\"NVIDIA_API_KEY\", \"\").startswith(\"nvapi-\"):\n",
    "    nvapi_key = getpass.getpass(\"Enter your NVIDIA API key: \")\n",
    "    assert nvapi_key.startswith(\"nvapi-\"), f\"{nvapi_key[:5]}... is not a valid key\"\n",
    "    os.environ[\"NVIDIA_API_KEY\"] = nvapi_key\n",
    "    os.environ[\"NGC_API_KEY\"] = nvapi_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc7eac2c-72d7-45be-9017-e43775eef914",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from os.path import expanduser\n",
    "home = expanduser(\"~\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edaa25d7-005c-4d26-a387-f752c799deac",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "try:\n",
    "    os.mkdir(f\"/local/.cache/nim\")\n",
    "    print(\"Cache Created\")\n",
    "except:\n",
    "    print(\"Cache Exists..\")\n",
    "    pass\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cb90e31-eff0-4ad7-96a3-eb00dadd2fff",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "os.environ['LOCAL_NIM_CACHE']=f\"/local/.cache/nim\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d8edd72-88f8-4b71-8655-3d10063d91ae",
   "metadata": {},
   "source": [
    "Let's run the NIM with a listed profile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70667fea-3e58-4c50-ae9c-a132fbed8bbb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "! docker run -it --rm --gpus all --shm-size=16GB \\\n",
    "-e NIM_MODEL_PROFILE=8d3824f766182a754159e88ad5a0bd465b1b4cf69ecf80bd6d6833753e945740 \\\n",
    "-e NGC_API_KEY \\\n",
    "-v $LOCAL_NIM_CACHE:/opt/nim/.cache \\\n",
    "-u $(id -u) -p $CONTAINER_PORT:8000  nvcr.io/nim/meta/llama3-8b-instruct:1.0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83ecdecd-41bb-41e4-8145-e93136047427",
   "metadata": {},
   "source": [
    "**Likely Output:**\n",
    "\n",
    "```\n",
    "\n",
    "===========================================\n",
    "== NVIDIA Inference Microservice LLM NIM ==\n",
    "===========================================\n",
    "\n",
    "NVIDIA Inference Microservice LLM NIM Version 1.0.0\n",
    "Model: nim/meta/llama3-8b-instruct\n",
    "\n",
    "Container image Copyright (c) 2016-2024, NVIDIA CORPORATION & AFFILIATES. All rights reserved.\n",
    "\n",
    "This NIM container is governed by the NVIDIA AI Product Agreement here:\n",
    "https://www.nvidia.com/en-us/data-center/products/nvidia-ai-enterprise/eula/.\n",
    "A copy of this license can be found under /opt/nim/LICENSE.\n",
    "\n",
    "The use of this model is governed by the AI Foundation Models Community License\n",
    "here: https://docs.nvidia.com/ai-foundation-models-community-license.pdf.\n",
    "\n",
    "ADDITIONAL INFORMATION: Meta Llama 3 Community License, Built with Meta Llama 3. \n",
    "A copy of the Llama 3 license can be found under /opt/nim/MODEL_LICENSE.\n",
    "\n",
    "2024-08-14 07:27:01,272 [INFO] PyTorch version 2.2.2 available.\n",
    "2024-08-14 07:27:02,170 [WARNING] [TRT-LLM] [W] Logger level already set from environment. Discard new verbosity: error\n",
    "2024-08-14 07:27:02,171 [INFO] [TRT-LLM] [I] Starting TensorRT-LLM init.\n",
    "2024-08-14 07:27:02,214 [INFO] [TRT-LLM] [I] TensorRT-LLM inited.\n",
    "[TensorRT-LLM] TensorRT-LLM version: 0.10.1.dev2024053000\n",
    "INFO 08-14 07:27:03.173 api_server.py:489] NIM LLM API version 1.0.0\n",
    "INFO 08-14 07:27:03.175 ngc_profile.py:217] Running NIM without LoRA. Only looking for compatible profiles that do not support LoRA.\n",
    "INFO 08-14 07:27:03.175 ngc_profile.py:219] Detected 0 compatible profile(s).\n",
    "INFO 08-14 07:27:03.175 ngc_profile.py:221] Detected additional 2 compatible profile(s) that are currently not runnable due to low free GPU memory.\n",
    "ERROR 08-14 07:27:03.176 utils.py:21] You are running NIM without LoRA, but the selected profile '8d3824f766182a754159e88ad5a0bd465b1b4cf69ecf80bd6d6833753e945740' has LoRA enabled. Please select a profile that does not have LoRA enabled, or alternatively, run NIM with LoRA enabled.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0e7e60e-a02e-47cc-abe2-1772171c6752",
   "metadata": {},
   "source": [
    "**The NIM log clearly indicates that we chose a LoRA enabled model profile, but passed no lora adapter definitions.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b63b1972-1f99-4aff-b7cb-e397a2478630",
   "metadata": {},
   "source": [
    "### LoRA Setup Overview\n",
    "\n",
    "You can extend a NIM to serve LoRA models using configuration defined in environment variables. However, a condition must be met:  The underlying NIM you use must be compatible with the base model of the LoRAs. Therefore, configuration is required. Further details are described below.\n",
    "\n",
    "#### LoRA Adapters\n",
    "\n",
    "- Download LoRA adapters from `NGC` or `Hugging Face`, or use your `custom LoRA adapters`. \n",
    "- LoRA adapters must be stored in separate directories, and one or more LoRA directories within the `LOCAL_PEFT_DIRECTORY` directory.\n",
    "- The names of the loaded LoRA adapters must match the name of the adapters’ directories. \n",
    "\n",
    "NIM for LLMs supports the NeMo and Hugging Face Transformers compatible formats.\n",
    "\n",
    "- A NeMo-formatted LoRA directory must contain one file with the `.nemo` extension. The name of the `.nemo` file does not need to match the name of its parent directory. The supported target modules are `[\"gate_proj\", \"o_proj\", \"up_proj\", \"down_proj\", \"k_proj\", \"q_proj\", \"v_proj\", \"attention_qkv\"]`.\n",
    "\n",
    "- LoRA adapters trained with Hugging Face Transformers are supported. The LoRA must contain an adapter_config.json file and one of {adapter_model.safetensors, adapter_model.bin} files. The supported target modules for NIM are `[\"gate_proj\", \"o_proj\", \"up_proj\", \"down_proj\", \"k_proj\", \"q_proj\", \"v_proj\"]`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "969fc84d-8c44-4d3a-b19b-e936d4632db8",
   "metadata": {},
   "source": [
    "#### LoRA Model Directory Structure\n",
    "\n",
    "For example, the directory used for storing one or more LoRAs (`LOCAL_PEFT_DIRECTORY`) should be organized as follows: \n",
    "- `loras` should the name of the directory you pass into the docker container as the value of `LOCAL_PEFT_DIRECTORY`. \n",
    "- Then the LoRAs that get loaded would be called `llama3-8b-math`, `llama3-8b-math-hf`, `llama3-8b-squad`, and `llama3-8b-squad-hf`.\n",
    "\n",
    "```text\n",
    "\n",
    "loras\n",
    "├── llama3-8b-math\n",
    "│   └── llama3_8b_math.nemo\n",
    "├── llama3-8b-math-hf\n",
    "│   ├── adapter_config.json\n",
    "│   └── adapter_model.bin\n",
    "├── llama3-8b-squad\n",
    "│   └── squad.nemo\n",
    "└── llama3-8b-squad-hf\n",
    "    ├── adapter_config.json\n",
    "    └── adapter_model.safetensors\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54516fd2-c764-44b4-9d7d-30cab84a22e2",
   "metadata": {},
   "source": [
    "### Downloading LoRA Adapters\n",
    "\n",
    "As mentioned above, you can download pre-trained adapters from the NVIDIA model registry NGC or HuggingFace.  Note that LoRA model weights are tied to a particular base model. You must only deploy LoRA models that were tuned with the same model as those being served by NIM. An example of how to download adapters from HuggingFace is given below.  \n",
    "\n",
    "*Please note that huggingface-cli login is a requirement.*\n",
    "\n",
    "```text\n",
    "#export and make directory\n",
    "export LOCAL_PEFT_DIRECTORY=~/loras\n",
    "mkdir $LOCAL_PEFT_DIRECTORY\n",
    "\n",
    "#download a LoRA from Hugging Face Hub\n",
    "mkdir $LOCAL_PEFT_DIRECTORY/llama3-lora\n",
    "huggingface-cli download <Hugging Face LoRA name> adapter_config.json adapter_model.safetensors --local-dir $LOCAL_PEFT_DIRECTORY/llama3-lora\n",
    "\n",
    "#create permissions\n",
    "chmod -R 777 $LOCAL_PEFT_DIRECTORY\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "Next, we demonstrate how to download LoRA adapters for `llama3-8b-instruct` from NGC."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6da8f70a-57b6-42cf-8965-30c0423b2c85",
   "metadata": {},
   "source": [
    "### Login to NVCR (NVIDIA Container Registry)\n",
    "\n",
    "To access a NIM docker image, you must login via `docker login nvcr.io.` This process requires a default username as `--username $oauthtoken` and `--password-stdin` that accepts the value of `$NGC_API_KEY.`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2941df99-daff-4035-a758-858ca24bdd11",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "! echo -e \"$NGC_API_KEY\" | docker login nvcr.io --username '$oauthtoken' --password-stdin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8132374-af76-4e96-8c41-dad5d6e3fea4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# create loras directory\n",
    "!mkdir -p loras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a1eec87-28c0-4c9a-b057-b7b0ff599e26",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# downloading vLLM-format loras\n",
    "# NOTE: Downloading LoRA adapter from NGC requires to have a config setup, the bootcamp has the LoRA pre-downloaded.\n",
    "# !ngc registry model download-version \"nim/meta/llama3-8b-instruct-lora:hf-math-v1\" --dest \"./loras/\" "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19c9cc73-059b-4fdd-8673-0a55fb0e9d7d",
   "metadata": {},
   "source": [
    "**Likely Output:** (if downloading from NGC)\n",
    "\n",
    "```text\n",
    "Downloaded 37.14 MB in 2s, Download speed: 18.57 MB/s               \n",
    "--------------------------------------------------------------------------------\n",
    "   Transfer id: llama3-8b-instruct-lora_vhf-math-v1\n",
    "   Download status: Completed\n",
    "   Downloaded local path: /home/<USER>/NIMs_Bootcamp/loras/llama3-8b-instruct-lora_vhf-math-v1-1\n",
    "   Total files downloaded: 4\n",
    "   Total downloaded size: 37.14 MB\n",
    "   Started at: 2024-08-15 12:37:20.924505\n",
    "   Completed at: 2024-08-15 12:37:22.928174\n",
    "   Duration taken: 2s\n",
    "   \n",
    "...\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94d2f756-3a67-45de-8fa2-f0e475be8bdf",
   "metadata": {},
   "source": [
    "### Adding LoRA Adapter to NIM\n",
    "\n",
    "Grant all view permissions for the LoRA files, for them to be accessible by docker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea1e6dbb-2361-4eb8-a8dc-03fe8cc2b552",
   "metadata": {},
   "outputs": [],
   "source": [
    "! chmod -R 777 loras/\n",
    "os.environ['NIM_PEFT_SOURCE'] = f'{os.getcwd()}/loras'\n",
    "os.environ['LOCAL_PEFT_DIRECTORY'] = f'{os.getcwd()}/loras'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fac06ad9-2f51-4292-9af1-3c0a296d515b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(os.environ['NIM_PEFT_SOURCE'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d50b6ec2-891c-4837-906d-47522b8e245e",
   "metadata": {},
   "source": [
    "As discussed above, lora configuration is sensitive to the files and folders structure, so to ensure correct deployment we get rid of the .ipynb checkpoints that might have crept in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c458b2b8-174c-40ef-b34f-daa7f657ddbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "! find . -type d -name \".ipynb_checkpoints\" -exec rm -rf {} +\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "984b323e-4cb6-4ab8-8223-8d36d01fa85a",
   "metadata": {},
   "source": [
    "Run the NIM docker container."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee02f36c-da74-4e51-9dda-e1a86cd59961",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['LORA_NIM_DOCKER']=\"nim_llm_with_lora\"\n",
    "! docker run -it -d --rm --gpus all --shm-size=16GB --name=$LORA_NIM_DOCKER   -e NGC_API_KEY  -e NIM_PEFT_SOURCE -v $LOCAL_PEFT_DIRECTORY:$NIM_PEFT_SOURCE -v $LOCAL_NIM_CACHE:/opt/nim/.cache  -u $(id -u) -p $CONTAINER_PORT:8000  nvcr.io/nim/meta/llama3-8b-instruct:1.0.0\n",
    "\n",
    "# In order to ensure, the local NIM container is completely loaded and doesn't remain in pending stage, we instantiate a wait interval. \n",
    "! sleep 60"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a6a6774-51e1-4912-8e02-e6a1a688e09f",
   "metadata": {},
   "source": [
    "Confirm that the NIM is running through the docker log."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "235bd541-f995-4359-9af9-ea390f289e33",
   "metadata": {},
   "outputs": [],
   "source": [
    "! docker logs --tail 45 $LORA_NIM_DOCKER"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfb262fe-aa00-42a7-95cd-7709677f405f",
   "metadata": {},
   "source": [
    "**Likely Output**:  \n",
    "\n",
    "```\n",
    "INFO 09-10 12:17:48.149 api_server.py:456] Serving endpoints:\n",
    "  0.0.0.0:8000/openapi.json\n",
    "  0.0.0.0:8000/docs\n",
    "  0.0.0.0:8000/docs/oauth2-redirect\n",
    "  0.0.0.0:8000/metrics\n",
    "  0.0.0.0:8000/v1/health/ready\n",
    "  0.0.0.0:8000/v1/health/live\n",
    "  0.0.0.0:8000/v1/models\n",
    "  0.0.0.0:8000/v1/version\n",
    "  0.0.0.0:8000/v1/chat/completions\n",
    "  0.0.0.0:8000/v1/completions\n",
    "INFO 09-10 12:17:48.149 api_server.py:460] An example cURL request:\n",
    "curl -X 'POST' \\\n",
    "  'http://0.0.0.0:8000/v1/chat/completions' \\\n",
    "  -H 'accept: application/json' \\\n",
    "  -H 'Content-Type: application/json' \\\n",
    "  -d '{\n",
    "    \"model\": \"meta/llama3-8b-instruct\",\n",
    "    \"messages\": [\n",
    "      {\n",
    "        \"role\":\"user\",\n",
    "        \"content\":\"Hello! How are you?\"\n",
    "      },\n",
    "      {\n",
    "        \"role\":\"assistant\",\n",
    "        \"content\":\"Hi! I am quite well, how can I help you today?\"\n",
    "      },\n",
    "      {\n",
    "        \"role\":\"user\",\n",
    "        \"content\":\"Can you write me a song?\"\n",
    "      }\n",
    "    ],\n",
    "    \"top_p\": 1,\n",
    "    \"n\": 1,\n",
    "    \"max_tokens\": 15,\n",
    "    \"stream\": true,\n",
    "    \"frequency_penalty\": 1.0,\n",
    "    \"stop\": [\"hello\"]\n",
    "  }'\n",
    "\n",
    "INFO 09-10 12:17:48.196 server.py:82] Started server process [33]\n",
    "INFO 09-10 12:17:48.197 on.py:48] Waiting for application startup.\n",
    "INFO 09-10 12:17:48.224 on.py:62] Application startup complete.\n",
    "INFO 09-10 12:17:48.225 server.py:214] Uvicorn running on http://0.0.0.0:8000 (Press CTRL+C to quit)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "466f28a6-f6e6-48c8-9b41-ac3d19949be7",
   "metadata": {},
   "source": [
    "### Running Inference\n",
    "\n",
    "We can quickly check the available model endpoint and hosted LoRA from the `/models` endpoint given via triton inference server in NIMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fbaf807-8f28-4ebd-a9ab-3644f7bd5017",
   "metadata": {},
   "outputs": [],
   "source": [
    "! curl -s http://0.0.0.0:${CONTAINER_PORT}/v1/models | jq"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01ded3cc-fbca-4a27-8f5a-e4f28c35702f",
   "metadata": {},
   "source": [
    "**Likely Output:**\n",
    "\n",
    "```json\n",
    "\n",
    "{\n",
    "  \"object\": \"list\",\n",
    "  \"data\": [\n",
    "    {\n",
    "      \"id\": \"meta/llama3-8b-instruct\",\n",
    "      \"object\": \"model\",\n",
    "      \"created\": 1723751020,\n",
    "      \"owned_by\": \"system\",\n",
    "      \"root\": \"meta/llama3-8b-instruct\",\n",
    "      \"parent\": null,\n",
    "      \"permission\": [\n",
    "        {\n",
    "          \"id\": \"modelperm-0f38baa00db14455846b31342c8b4367\",\n",
    "          \"object\": \"model_permission\",\n",
    "          \"created\": 1723751020,\n",
    "          \"allow_create_engine\": false,\n",
    "          \"allow_sampling\": true,\n",
    "          \"allow_logprobs\": true,\n",
    "          \"allow_search_indices\": false,\n",
    "          \"allow_view\": true,\n",
    "          \"allow_fine_tuning\": false,\n",
    "          \"organization\": \"*\",\n",
    "          \"group\": null,\n",
    "          \"is_blocking\": false\n",
    "        }\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"id\": \"llama3-8b-instruct-lora_vhf-math-v1\",\n",
    "      \"object\": \"model\",\n",
    "      \"created\": 1723751020,\n",
    "      \"owned_by\": \"system\",\n",
    "      \"root\": \"meta/llama3-8b-instruct\",\n",
    "      \"parent\": null,\n",
    "      \"permission\": [     \n",
    "    ...   \n",
    "  ]\n",
    "}\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d485906-8ced-4306-9b79-d743a4031eaa",
   "metadata": {},
   "source": [
    "Running a simple inference request."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ac9f7d6-c184-440a-9967-cd051665e7fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_nvidia_ai_endpoints import ChatNVIDIA\n",
    "\n",
    "model = \"llama3-8b-instruct-lora_vhf-math-v1\" # \"meta/llama-3.1-70b-instruct\"\n",
    "llm = ChatNVIDIA(base_url=\"http://0.0.0.0:{}/v1\".format(os.environ['CONTAINER_PORT']),model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c765fe2b-4a97-4ef1-9622-77658e73a6ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "out = llm.invoke(\"What is 7+9?\")\n",
    "\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feab09bf-af73-43cc-9e61-4a78d89396ea",
   "metadata": {},
   "source": [
    "**Likely Output:**\n",
    "```python\n",
    "AIMessage(content='7 + 9 = 16', response_metadata={'role': 'assistant', 'content': '7 + 9 = 16', 'token_usage': {'prompt_tokens': 17, 'total_tokens': 25, 'completion_tokens': 8}, 'finish_reason': 'stop', 'model_name': 'llama3-8b-instruct-lora_vhf-math-v1'}, id='run-c52c2e00-80d4-409b-88c1-8b977b6e9041-0', role='assistant')\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7b7dda3-f72a-43bb-8f5d-9d7ab70e8caf",
   "metadata": {},
   "outputs": [],
   "source": [
    "out = llm.invoke(\"Find the coordinates of the point halfway between the points $(3,7)$ and $(5,1)$.\")\n",
    "\n",
    "print(out.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82005e6c-17cb-4878-b8fb-97d162360a3e",
   "metadata": {},
   "source": [
    "Stop all runinng docker containers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bea97ef-ba81-4383-a8c4-8fa6a43dac8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!docker container stop $LORA_NIM_DOCKER "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12afb511-ca0f-4bba-96de-fd653f8a5bb0",
   "metadata": {},
   "source": [
    "### Creating Custom LoRA Adapter \n",
    "\n",
    "To use a custom LoRA adapter, you must build one using the <a href=\"custom_lora.ipynb\"> custom_lora notebook </a>. After completing the process, please deploy the custom LoRA adapter with NIM by running the cells below.  The first step would be to copy the adapter to `loras` directory. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "393fa191-940a-4df6-b260-0658107e03da",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cp  -r model/Llama-3-8b-instruct-hf-finetune loras "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a581e7c-e2ef-4215-8e0a-1665c21e5bf0",
   "metadata": {},
   "source": [
    "Create permissions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d496f44-5038-45dc-8770-98395e6f73a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "!chmod -R 777 loras/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "998fcd43-ed4e-40cf-98b5-1d1123e41cbd",
   "metadata": {},
   "source": [
    "Run the NIM docker container."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db469796-2e21-4a5c-888b-15484d0bf218",
   "metadata": {},
   "outputs": [],
   "source": [
    "! docker run -it --rm -d --gpus all --shm-size=16GB   -e NGC_API_KEY  -e NIM_PEFT_SOURCE -v $LOCAL_PEFT_DIRECTORY:$NIM_PEFT_SOURCE -v $LOCAL_NIM_CACHE:/opt/nim/.cache  -u $(id -u) -p $CONTAINER_PORT:8000  nvcr.io/nim/meta/llama3-8b-instruct:1.0.0\n",
    "! sleep 60 # wait for container to load fully"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f32dbfb2-adea-480a-8a9a-063fd647653d",
   "metadata": {},
   "outputs": [],
   "source": [
    "! docker logs --tail 50 $(docker ps -q -l) # view the last logs from the container"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b7936a5-f76e-4aa3-88d5-28da25930c13",
   "metadata": {},
   "source": [
    "### Running Inference with Custom LoRA Adapter\n",
    "\n",
    "We can quickly check the available model endpoint and hosted LoRA from the `/models` endpoint given via triton inference server in NIMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18d04883-cdd5-4cc9-875e-6a5e18564104",
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl -s http://0.0.0.0:${CONTAINER_PORT}/v1/models | jq"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf8e9bd6-a665-4332-b65a-1e73670a88d1",
   "metadata": {},
   "source": [
    "Test to confirm the model is reachable and running"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2bf7fc0-e094-4833-86fe-bd0ca2987af3",
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl -X 'POST' \\\n",
    "    \"http://0.0.0.0:${CONTAINER_PORT}/v1/completions\" \\\n",
    "    -H 'accept: application/json' \\\n",
    "    -H 'Content-Type: application/json' \\\n",
    "    -d '{\"model\": \"Llama-3-8b-instruct-hf-finetune\", \\\n",
    "         \"prompt\": \"Once upon a time\",\\\n",
    "         \"max_tokens\": 64        }'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8bd4e4f-c190-477d-8752-b286e986e0cf",
   "metadata": {},
   "source": [
    "Running a simple inference request.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67b118b8-4310-4b85-b80a-18554f45be56",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_nvidia_ai_endpoints import ChatNVIDIA\n",
    "\n",
    "model = \"Llama-3-8b-instruct-hf-finetune\" # using custom LoRA adapter \n",
    "\n",
    "llm = ChatNVIDIA(base_url=\"http://0.0.0.0:{}/v1\".format(os.environ['CONTAINER_PORT']),model=model, temperature=0.1, max_tokens=200, top_p=1.0 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1523d48-7236-4e0a-995c-8bbfc176f743",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = llm.invoke(\"explain what is astrophotography?\")\n",
    "print(output.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06457663-b5cc-4c9a-af46-7dd7e4fd765f",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## References\n",
    "\n",
    "- https://docs.nvidia.com/nim/large-language-models/latest/peft.html\n",
    "\n",
    "## Licensing\n",
    "\n",
    "Copyright © 2024 OpenACC-Standard.org. This material is released by OpenACC-Standard.org, in collaboration with NVIDIA Corporation, under the Creative Commons Attribution 4.0 International (CC BY 4.0). These materials include references to hardware and software developed by other entities; all applicable licensing and copyrights apply."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58d20729-200c-4e2a-88f9-4c700c57f43f",
   "metadata": {},
   "source": [
    "<br>\n",
    "<div>\n",
    "    <span style=\"float: left; width: 33%; text-align: left;\"><a href=\"locally_deployed_nim.ipynb\">Previous Notebook</a></span>\n",
    "    <span style=\"float: left; width: 34%; text-align: center;\">\n",
    "        <a href=\"rag_nim_endpoints.ipynb\">1</a>\n",
    "        <a href=\"locally_deployed_nim.ipynb\">2</a>\n",
    "        <a>3</a>\n",
    "        <!-- <a href=\"challenge.ipynb\">4</a> -->\n",
    "    </span>\n",
    "    <!-- <span style=\"float: left; width: 33%; text-align: right;\"><a href=\"challenge.ipynb\">Next Notebook</a></span> -->\n",
    "</div>\n",
    "\n",
    "<br>\n",
    "<p> <center> <a href=\"../../Start-NIM-RAG.ipynb\">Home Page</a> </center> </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00be7543-f603-44ca-ad41-2a067e413313",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Bootcamp_Kernel",
   "language": "python",
   "name": "bootcamp_kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
